{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Neural Network Baseline with Modular Pipeline\n",
        "\n",
        "This notebook demonstrates the modular neural network training approach.\n",
        "\n",
        "## Features\n",
        "- Imports reusable pipeline modules\n",
        "- Data preparation stays in notebook (feature engineering experimentation)\n",
        "- Model training via imported functions\n",
        "- Model saving with automatic versioning\n",
        "- Submission server integration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Import pipeline modules for reusable functionality.\n",
        "Data preparation and feature engineering remain in the notebook for experimentation.\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import polars as pl\n",
        "from pathlib import Path\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from pipeline.config import Config, set_seed\n",
        "from pipeline.models import SeqModel, TemporalHuber, prepare_targets\n",
        "from pipeline.training import train_model\n",
        "from pipeline.save_model import save_model_ensemble, load_model_ensemble\n",
        "from pipeline.submission_server import create_submission_server\n",
        "from pipeline.utils import timer, print_timing_summary\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import GroupKFold\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "set_seed(42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configuration:\n",
            "  Data directory: /Users/matth/projects/nfl-big-data-bowl-2026-prediction/data/raw/train_data\n",
            "  Window size: 10\n",
            "  Max future horizon: 94\n",
            "  Number of folds: 5\n",
            "  Device: cpu\n"
          ]
        }
      ],
      "source": [
        "# Initialize configuration\n",
        "config = Config()\n",
        "\n",
        "print(\"Configuration:\")\n",
        "print(f\"  Data directory: {config.DATA_DIR}\")\n",
        "print(f\"  Window size: {config.WINDOW_SIZE}\")\n",
        "print(f\"  Max future horizon: {config.MAX_FUTURE_HORIZON}\")\n",
        "print(f\"  Number of folds: {config.N_FOLDS}\")\n",
        "print(f\"  Device: {config.DEVICE}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Loading\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data preparation would go here\n"
          ]
        }
      ],
      "source": [
        "# Load training data\n",
        "# TODO: Implement data loading and sequence preparation here\n",
        "# This is where feature engineering happens - keep it in notebook for experimentation\n",
        "\n",
        "# Example structure:\n",
        "train_input_files = [config.DATA_DIR / f\"input/input_2023_w{w:02d}.csv\" for w in range(1, 19)]\n",
        "train_output_files = [config.DATA_DIR / f\"output/output_2023_w{w:02d}.csv\" for w in range(1, 19)]\n",
        "train_input = pd.concat([pd.read_csv(f) for f in train_input_files if f.exists()])\n",
        "train_output = pd.concat([pd.read_csv(f) for f in train_output_files if f.exists()])\n",
        "\n",
        "print(\"Data preparation would go here\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def height_to_feet(height_str):\n",
        "    try:\n",
        "        ft, inches = map(int, str(height_str).split('-'))\n",
        "        return ft + inches/12\n",
        "    except:\n",
        "        return 6.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def add_advanced_features(df):\n",
        "    \"\"\"Original nflnn.py features\"\"\"\n",
        "    print(\"Adding advanced features...\")\n",
        "    df = df.copy()\n",
        "    df = df.sort_values(['game_id', 'play_id', 'nfl_id', 'frame_id'])\n",
        "    gcols = ['game_id', 'play_id', 'nfl_id']\n",
        "    \n",
        "    # Distance rate features (3)\n",
        "    if 'distance_to_ball' in df.columns:\n",
        "        df['distance_to_ball_change'] = df.groupby(gcols)['distance_to_ball'].diff().fillna(0)\n",
        "        df['distance_to_ball_accel'] = df.groupby(gcols)['distance_to_ball_change'].diff().fillna(0)\n",
        "        df['time_to_intercept'] = (df['distance_to_ball'] / \n",
        "                                    (np.abs(df['distance_to_ball_change']) + 0.1)).clip(0, 10)\n",
        "    \n",
        "    # Target alignment features (3)\n",
        "    if 'ball_direction_x' in df.columns:\n",
        "        df['velocity_alignment'] = (\n",
        "            df['velocity_x'] * df['ball_direction_x'] +\n",
        "            df['velocity_y'] * df['ball_direction_y']\n",
        "        )\n",
        "        df['velocity_perpendicular'] = (\n",
        "            df['velocity_x'] * (-df['ball_direction_y']) +\n",
        "            df['velocity_y'] * df['ball_direction_x']\n",
        "        )\n",
        "        if 'acceleration_x' in df.columns:\n",
        "            df['accel_alignment'] = (\n",
        "                df['acceleration_x'] * df['ball_direction_x'] +\n",
        "                df['acceleration_y'] * df['ball_direction_y']\n",
        "            )\n",
        "    \n",
        "    # Multi-window rolling (24)\n",
        "    for window in [3, 5, 10]:\n",
        "        for col in ['velocity_x', 'velocity_y', 's', 'a']:\n",
        "            if col in df.columns:\n",
        "                df[f'{col}_roll{window}'] = df.groupby(gcols)[col].transform(\n",
        "                    lambda x: x.rolling(window, min_periods=1).mean()\n",
        "                )\n",
        "                df[f'{col}_std{window}'] = df.groupby(gcols)[col].transform(\n",
        "                    lambda x: x.rolling(window, min_periods=1).std()\n",
        "                ).fillna(0)\n",
        "    \n",
        "    # Extended lag features (8)\n",
        "    for lag in [4, 5]:\n",
        "        for col in ['x', 'y', 'velocity_x', 'velocity_y']:\n",
        "            if col in df.columns:\n",
        "                df[f'{col}_lag{lag}'] = df.groupby(gcols)[col].shift(lag).fillna(0)\n",
        "    \n",
        "    # Velocity change features (4)\n",
        "    if 'velocity_x' in df.columns:\n",
        "        df['velocity_x_change'] = df.groupby(gcols)['velocity_x'].diff().fillna(0)\n",
        "        df['velocity_y_change'] = df.groupby(gcols)['velocity_y'].diff().fillna(0)\n",
        "        df['speed_change'] = df.groupby(gcols)['s'].diff().fillna(0)\n",
        "        df['direction_change'] = df.groupby(gcols)['dir'].diff().fillna(0)\n",
        "        df['direction_change'] = df['direction_change'].apply(\n",
        "            lambda x: x if abs(x) < 180 else x - 360 * np.sign(x)\n",
        "        )\n",
        "    \n",
        "    # Field position features (4)\n",
        "    df['dist_from_left'] = df['y']\n",
        "    df['dist_from_right'] = 53.3 - df['y']\n",
        "    df['dist_from_sideline'] = np.minimum(df['dist_from_left'], df['dist_from_right'])\n",
        "    df['dist_from_endzone'] = np.minimum(df['x'], 120 - df['x'])\n",
        "    \n",
        "    # Role-specific features (3)\n",
        "    if 'is_receiver' in df.columns and 'velocity_alignment' in df.columns:\n",
        "        df['receiver_optimality'] = df['is_receiver'] * df['velocity_alignment']\n",
        "        df['receiver_deviation'] = df['is_receiver'] * np.abs(df.get('velocity_perpendicular', 0))\n",
        "    if 'is_coverage' in df.columns and 'closing_speed' in df.columns:\n",
        "        df['defender_closing_speed'] = df['is_coverage'] * df['closing_speed']\n",
        "    \n",
        "    # Time features (2)\n",
        "    df['frames_elapsed'] = df.groupby(gcols).cumcount()\n",
        "    df['normalized_time'] = df.groupby(gcols)['frames_elapsed'].transform(\n",
        "        lambda x: x / (x.max() + 1)\n",
        "    )\n",
        "    \n",
        "    print(f\"Total features after enhancement: {len(df.columns)}\")\n",
        "    \n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def add_safe_catboost_features(df):\n",
        "    \"\"\"ðŸŽ¯ ONLY SAFE, GENERALIZABLE ADDITIONS\"\"\"\n",
        "    print(\"Adding safe CatBoost features (physics-based only)...\")\n",
        "    \n",
        "    # Player BMI\n",
        "    height_parts = df['player_height'].str.split('-', expand=True)\n",
        "    df['height_inches'] = height_parts[0].astype(float) * 12 + height_parts[1].astype(float)\n",
        "    df['bmi'] = (df['player_weight'] / (df['height_inches']**2)) * 703\n",
        "    \n",
        "    # Orientation alignment\n",
        "    df['orientation_diff'] = np.abs(df['o'] - df['dir'])\n",
        "    df['orientation_diff'] = np.minimum(df['orientation_diff'], 360 - df['orientation_diff'])\n",
        "    \n",
        "    # Non-linear physics\n",
        "    df['speed_squared'] = df['s'] ** 2\n",
        "    df['dist_squared'] = df['distance_to_ball'] ** 2\n",
        "    \n",
        "    # Angle to target\n",
        "    df['angle_diff'] = np.abs(df['o'] - np.degrees(df['angle_to_ball']))\n",
        "    df['angle_diff'] = np.minimum(df['angle_diff'], 360 - df['angle_diff'])\n",
        "    \n",
        "    # Better closing speed calculation\n",
        "    df['velocity_toward_ball'] = (\n",
        "        df['velocity_x'] * np.cos(df['angle_to_ball']) + \n",
        "        df['velocity_y'] * np.sin(df['angle_to_ball'])\n",
        "    )\n",
        "    \n",
        "    print(\"Added 6 safe physics features\")\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_opponent_proximity_simple(input_df):\n",
        "    \"\"\"Only basic opponent proximity - no complex features\"\"\"\n",
        "    features = []\n",
        "    \n",
        "    for (gid, pid), group in tqdm(input_df.groupby(['game_id', 'play_id']), \n",
        "                                   desc=\"ðŸˆ Opponent proximity\", leave=False):\n",
        "        last = group.sort_values('frame_id').groupby('nfl_id').last()\n",
        "        \n",
        "        if len(last) < 2:\n",
        "            continue\n",
        "            \n",
        "        positions = last[['x', 'y']].values\n",
        "        sides = last['player_side'].values\n",
        "        speeds = last['s'].values\n",
        "        directions = last['dir'].values\n",
        "        \n",
        "        for i, (nid, side) in enumerate(zip(last.index, sides)):\n",
        "            opp_mask = sides != side\n",
        "            \n",
        "            feat = {\n",
        "                'game_id': gid, 'play_id': pid, 'nfl_id': nid,\n",
        "                'nearest_opp_dist': 50.0,\n",
        "                'num_nearby_opp_3': 0,\n",
        "                'num_nearby_opp_5': 0,\n",
        "                'closing_speed_opp': 0.0,\n",
        "            }\n",
        "            \n",
        "            if not opp_mask.any():\n",
        "                features.append(feat)\n",
        "                continue\n",
        "            \n",
        "            opp_positions = positions[opp_mask]\n",
        "            distances = np.sqrt(((positions[i] - opp_positions)**2).sum(axis=1))\n",
        "            \n",
        "            if len(distances) == 0:\n",
        "                features.append(feat)\n",
        "                continue\n",
        "                \n",
        "            nearest_idx = distances.argmin()\n",
        "            feat['nearest_opp_dist'] = distances[nearest_idx]\n",
        "            feat['num_nearby_opp_3'] = (distances < 3.0).sum()\n",
        "            feat['num_nearby_opp_5'] = (distances < 5.0).sum()\n",
        "            \n",
        "            # Simple closing speed\n",
        "            my_vx = speeds[i] * np.sin(np.deg2rad(directions[i]))\n",
        "            my_vy = speeds[i] * np.cos(np.deg2rad(directions[i]))\n",
        "            opp_speeds = speeds[opp_mask]\n",
        "            opp_dirs = directions[opp_mask]\n",
        "            opp_vx = opp_speeds[nearest_idx] * np.sin(np.deg2rad(opp_dirs[nearest_idx]))\n",
        "            opp_vy = opp_speeds[nearest_idx] * np.cos(np.deg2rad(opp_dirs[nearest_idx]))\n",
        "            \n",
        "            rel_vx = my_vx - opp_vx\n",
        "            rel_vy = my_vy - opp_vy\n",
        "            to_me = positions[i] - opp_positions[nearest_idx]\n",
        "            to_me_norm = to_me / (np.linalg.norm(to_me) + 0.1)\n",
        "            feat['closing_speed_opp'] = -(rel_vx * to_me_norm[0] + rel_vy * to_me_norm[1])\n",
        "            \n",
        "            features.append(feat)\n",
        "    \n",
        "    return pd.DataFrame(features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def add_basic_features(input_df):\n",
        "    input_df = input_df.copy()\n",
        "    input_df['player_height_feet'] = input_df['player_height'].apply(height_to_feet)\n",
        "    \n",
        "    dir_rad = np.deg2rad(input_df['dir'].fillna(0))\n",
        "    delta_t = 0.1\n",
        "    input_df['velocity_x'] = (input_df['s'] + 0.5 * input_df['a'] * delta_t) * np.sin(dir_rad)\n",
        "    input_df['velocity_y'] = (input_df['s'] + 0.5 * input_df['a'] * delta_t) * np.cos(dir_rad)\n",
        "    input_df['acceleration_x'] = input_df['a'] * np.sin(dir_rad)\n",
        "    input_df['acceleration_y'] = input_df['a'] * np.cos(dir_rad)\n",
        "    \n",
        "    # Roles\n",
        "    input_df['is_offense'] = (input_df['player_side'] == 'Offense').astype(int)\n",
        "    input_df['is_defense'] = (input_df['player_side'] == 'Defense').astype(int)\n",
        "    input_df['is_receiver'] = (input_df['player_role'] == 'Targeted Receiver').astype(int)\n",
        "    input_df['is_coverage'] = (input_df['player_role'] == 'Defensive Coverage').astype(int)\n",
        "    input_df['is_passer'] = (input_df['player_role'] == 'Passer').astype(int)\n",
        "    \n",
        "    # Physics\n",
        "    mass_kg = input_df['player_weight'].fillna(200.0) / 2.20462\n",
        "    input_df['momentum_x'] = input_df['velocity_x'] * mass_kg\n",
        "    input_df['momentum_y'] = input_df['velocity_y'] * mass_kg\n",
        "    input_df['kinetic_energy'] = 0.5 * mass_kg * (input_df['s'] ** 2)\n",
        "    \n",
        "    # Ball features\n",
        "    if 'ball_land_x' in input_df.columns:\n",
        "        ball_dx = input_df['ball_land_x'] - input_df['x']\n",
        "        ball_dy = input_df['ball_land_y'] - input_df['y']\n",
        "        input_df['distance_to_ball'] = np.sqrt(ball_dx**2 + ball_dy**2)\n",
        "        input_df['angle_to_ball'] = np.arctan2(ball_dy, ball_dx)\n",
        "        input_df['ball_direction_x'] = ball_dx / (input_df['distance_to_ball'] + 1e-6)\n",
        "        input_df['ball_direction_y'] = ball_dy / (input_df['distance_to_ball'] + 1e-6)\n",
        "        input_df['closing_speed'] = (\n",
        "            input_df['velocity_x'] * input_df['ball_direction_x'] +\n",
        "            input_df['velocity_y'] * input_df['ball_direction_y']\n",
        "        )\n",
        "    \n",
        "    # Sort for temporal\n",
        "    input_df = input_df.sort_values(['game_id', 'play_id', 'nfl_id', 'frame_id'])\n",
        "    gcols = ['game_id', 'play_id', 'nfl_id']\n",
        "    \n",
        "    # Original lag features (1-3)\n",
        "    for lag in [1, 2, 3]:\n",
        "        input_df[f'x_lag{lag}'] = input_df.groupby(gcols)['x'].shift(lag)\n",
        "        input_df[f'y_lag{lag}'] = input_df.groupby(gcols)['y'].shift(lag)\n",
        "        input_df[f'velocity_x_lag{lag}'] = input_df.groupby(gcols)['velocity_x'].shift(lag)\n",
        "        input_df[f'velocity_y_lag{lag}'] = input_df.groupby(gcols)['velocity_y'].shift(lag)\n",
        "    \n",
        "    # EMA features\n",
        "    input_df['velocity_x_ema'] = input_df.groupby(gcols)['velocity_x'].transform(\n",
        "        lambda x: x.ewm(alpha=0.3, adjust=False).mean()\n",
        "    )\n",
        "    input_df['velocity_y_ema'] = input_df.groupby(gcols)['velocity_y'].transform(\n",
        "        lambda x: x.ewm(alpha=0.3, adjust=False).mean()\n",
        "    )\n",
        "    input_df['speed_ema'] = input_df.groupby(gcols)['s'].transform(\n",
        "        lambda x: x.ewm(alpha=0.3, adjust=False).mean()\n",
        "    )\n",
        "\n",
        "    return input_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_feature_cols(input_df):\n",
        "    feature_cols = [\n",
        "        # Core (9)\n",
        "        'x', 'y', 's', 'a', 'o', 'dir', 'frame_id', 'ball_land_x', 'ball_land_y',\n",
        "        \n",
        "        # Player (4 - added height_inches, bmi)\n",
        "        'player_height_feet', 'player_weight', 'height_inches', 'bmi',\n",
        "        \n",
        "        # Motion (7 - added speed_squared)\n",
        "        'velocity_x', 'velocity_y', 'acceleration_x', 'acceleration_y',\n",
        "        'momentum_x', 'momentum_y', 'kinetic_energy', 'speed_squared',\n",
        "        \n",
        "        # Roles (5)\n",
        "        'is_offense', 'is_defense', 'is_receiver', 'is_coverage', 'is_passer',\n",
        "        \n",
        "        # Ball (7 - added dist_squared, velocity_toward_ball)\n",
        "        'distance_to_ball', 'dist_squared', 'angle_to_ball', \n",
        "        'ball_direction_x', 'ball_direction_y', 'closing_speed', 'velocity_toward_ball',\n",
        "        \n",
        "        # Angles (2 - NEW)\n",
        "        'orientation_diff', 'angle_diff',\n",
        "        \n",
        "        # Opponent (4 - NEW)\n",
        "        'nearest_opp_dist', 'num_nearby_opp_3', 'num_nearby_opp_5', 'closing_speed_opp',\n",
        "        \n",
        "        # Original temporal (15)\n",
        "        'x_lag1', 'y_lag1', 'velocity_x_lag1', 'velocity_y_lag1',\n",
        "        'x_lag2', 'y_lag2', 'velocity_x_lag2', 'velocity_y_lag2',\n",
        "        'x_lag3', 'y_lag3', 'velocity_x_lag3', 'velocity_y_lag3',\n",
        "        'velocity_x_ema', 'velocity_y_ema', 'speed_ema',\n",
        "        \n",
        "        # Distance rate (3)\n",
        "        'distance_to_ball_change', 'distance_to_ball_accel', 'time_to_intercept',\n",
        "        \n",
        "        # Target alignment (3)\n",
        "        'velocity_alignment', 'velocity_perpendicular', 'accel_alignment',\n",
        "        \n",
        "        # Multi-window rolling (24)\n",
        "        'velocity_x_roll3', 'velocity_x_std3', 'velocity_y_roll3', 'velocity_y_std3',\n",
        "        's_roll3', 's_std3', 'a_roll3', 'a_std3',\n",
        "        'velocity_x_roll5', 'velocity_x_std5', 'velocity_y_roll5', 'velocity_y_std5',\n",
        "        's_roll5', 's_std5', 'a_roll5', 'a_std5',\n",
        "        'velocity_x_roll10', 'velocity_x_std10', 'velocity_y_roll10', 'velocity_y_std10',\n",
        "        's_roll10', 's_std10', 'a_roll10', 'a_std10',\n",
        "        \n",
        "        # Extended lags (8)\n",
        "        'x_lag4', 'y_lag4', 'velocity_x_lag4', 'velocity_y_lag4',\n",
        "        'x_lag5', 'y_lag5', 'velocity_x_lag5', 'velocity_y_lag5',\n",
        "        \n",
        "        # Velocity changes (4)\n",
        "        'velocity_x_change', 'velocity_y_change', 'speed_change', 'direction_change',\n",
        "        \n",
        "        # Field position (2)\n",
        "        'dist_from_sideline', 'dist_from_endzone',\n",
        "        \n",
        "        # Role-specific (3)\n",
        "        'receiver_optimality', 'receiver_deviation', 'defender_closing_speed',\n",
        "        \n",
        "        # Time (2)\n",
        "        'frames_elapsed', 'normalized_time',\n",
        "    ]\n",
        "    # Check for missing columns\n",
        "    missing_cols = [col for col in feature_cols if col not in input_df.columns]\n",
        "    existing_cols = [col for col in feature_cols if col in input_df.columns]\n",
        "\n",
        "    if missing_cols:\n",
        "        print(f\"âš ï¸  Missing columns ({len(missing_cols)}):\")\n",
        "        for col in missing_cols:\n",
        "            print(f\"    - {col}\")\n",
        "        print(f\"\\nâœ“ Using {len(existing_cols)} existing columns out of {len(feature_cols)} total\")\n",
        "    else:\n",
        "        print(f\"âœ“ All {len(feature_cols)} feature columns found in input_df\")\n",
        "\n",
        "    feature_cols = existing_cols\n",
        "    return feature_cols"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_sequences(input_df, feature_cols, output_df=None, test_template=None, is_training=True, window_size=8):\n",
        "    # CREATE SEQUENCES\n",
        "    input_df.set_index(['game_id', 'play_id', 'nfl_id'], inplace=True)\n",
        "    grouped = input_df.groupby(level=['game_id', 'play_id', 'nfl_id'])\n",
        "    \n",
        "    target_rows = output_df if is_training else test_template\n",
        "    target_groups = target_rows[['game_id', 'play_id', 'nfl_id']].drop_duplicates()\n",
        "    \n",
        "    sequences, targets_dx, targets_dy, targets_frame_ids, sequence_ids = [], [], [], [], []\n",
        "    \n",
        "    for _, row in tqdm(target_groups.iterrows(), total=len(target_groups), desc=\"Creating sequences\"):\n",
        "        key = (row['game_id'], row['play_id'], row['nfl_id'])\n",
        "        \n",
        "        try:\n",
        "            group_df = grouped.get_group(key)\n",
        "        except KeyError:\n",
        "            continue\n",
        "        \n",
        "        input_window = group_df.tail(window_size)\n",
        "        \n",
        "        if len(input_window) < window_size:\n",
        "            if is_training:\n",
        "                continue\n",
        "            pad_len = window_size - len(input_window)\n",
        "            pad_df = pd.DataFrame(np.nan, index=range(pad_len), columns=input_window.columns)\n",
        "            input_window = pd.concat([pad_df, input_window], ignore_index=True)\n",
        "        \n",
        "        input_window = input_window.fillna(group_df.mean(numeric_only=True))\n",
        "        seq = input_window[feature_cols].values\n",
        "        \n",
        "        if np.isnan(seq).any():\n",
        "            if is_training:\n",
        "                continue\n",
        "            seq = np.nan_to_num(seq, nan=0.0)\n",
        "        \n",
        "        sequences.append(seq)\n",
        "        \n",
        "        if is_training:\n",
        "            out_grp = output_df[\n",
        "                (output_df['game_id']==row['game_id']) &\n",
        "                (output_df['play_id']==row['play_id']) &\n",
        "                (output_df['nfl_id']==row['nfl_id'])\n",
        "            ].sort_values('frame_id')\n",
        "            \n",
        "            last_x = input_window.iloc[-1]['x']\n",
        "            last_y = input_window.iloc[-1]['y']\n",
        "            \n",
        "            dx = out_grp['x'].values - last_x\n",
        "            dy = out_grp['y'].values - last_y\n",
        "            \n",
        "            targets_dx.append(dx)\n",
        "            targets_dy.append(dy)\n",
        "            targets_frame_ids.append(out_grp['frame_id'].values)\n",
        "        \n",
        "        sequence_ids.append({\n",
        "            'game_id': key[0],\n",
        "            'play_id': key[1],\n",
        "            'nfl_id': key[2],\n",
        "            'frame_id': input_window.iloc[-1]['frame_id']\n",
        "        })\n",
        "    \n",
        "    print(f\"Created {len(sequences)} sequences with {len(feature_cols)} features each\")\n",
        "    \n",
        "    if is_training:\n",
        "        return sequences, targets_dx, targets_dy, targets_frame_ids, sequence_ids\n",
        "    return sequences, sequence_ids\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def feature_engineering(input_df, timings):\n",
        "    \"\"\"\n",
        "    Prepare sequences with enhanced features (original + safe additions)\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"PREPARING SEQUENCES WITH ENHANCED FEATURES\")\n",
        "    \n",
        "    input_df = input_df.copy()\n",
        "    \n",
        "    # Basic features\n",
        "    print(\"Step 1/4: Adding basic features...\")\n",
        "    with timer(\"add_basic_features\", timings):\n",
        "        input_df = add_basic_features(input_df)\n",
        "    \n",
        "    # Advanced features (original)\n",
        "    print(\"Step 2/4: Adding advanced features...\")\n",
        "    with timer(\"add_advanced_features\", timings):\n",
        "        input_df = add_advanced_features(input_df)\n",
        "    \n",
        "    # Safe CatBoost additions\n",
        "    print(\"Step 3/4: Adding safe CatBoost features...\")\n",
        "    with timer(\"add_safe_catboost_features\", timings):\n",
        "        input_df = add_safe_catboost_features(input_df)\n",
        "    \n",
        "    # Opponent proximity (simple)\n",
        "    print(\"Step 3.5/4: Adding opponent proximity...\")\n",
        "    with timer(\"get_opponent_proximity_simple\", timings):\n",
        "        opp_features = get_opponent_proximity_simple(input_df)\n",
        "        input_df = input_df.merge(opp_features, on=['game_id', 'play_id', 'nfl_id'], how='left')\n",
        "    \n",
        "    # Feature list\n",
        "    print(\"Step 4/4: Creating sequences...\")\n",
        "    with timer(\"get_feature_cols\", timings):\n",
        "        feature_cols = get_feature_cols(input_df)\n",
        "\n",
        "    return input_df, feature_cols, timings\n",
        "    \n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[2/4] Preparing with enhanced features...\n",
            "\n",
            "================================================================================\n",
            "PREPARING SEQUENCES WITH ENHANCED FEATURES\n",
            "Step 1/4: Adding basic features...\n",
            "  [32.64s] add_basic_features\n",
            "Step 2/4: Adding advanced features...\n",
            "Adding advanced features...\n",
            "Total features after enhancement: 107\n",
            "  [270.18s] add_advanced_features\n",
            "Step 3/4: Adding safe CatBoost features...\n",
            "Adding safe CatBoost features (physics-based only)...\n",
            "Added 6 safe physics features\n",
            "  [3.25s] add_safe_catboost_features\n",
            "Step 3.5/4: Adding opponent proximity...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a34738a25cf1454c90ff403eaada6d0f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "ðŸˆ Opponent proximity:   0%|          | 0/14108 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  [47.72s] get_opponent_proximity_simple\n",
            "Step 4/4: Creating sequences...\n",
            "âœ“ All 103 feature columns found in input_df\n",
            "  [0.00s] get_feature_cols\n",
            "  [354.40s] feature_engineering\n",
            "\n",
            "Feature Engineering Summary:\n",
            "============================================================\n",
            "  feature_engineering      : 354.40s ( 50.0%)\n",
            "  add_advanced_features    : 270.18s ( 38.1%)\n",
            "  get_opponent_proximity_simple:  47.72s (  6.7%)\n",
            "  add_basic_features       :  32.64s (  4.6%)\n",
            "  add_safe_catboost_features:   3.25s (  0.5%)\n",
            "  get_feature_cols         :   0.00s (  0.0%)\n",
            "Total                    : 708.19s\n"
          ]
        }
      ],
      "source": [
        "feature_engineering_timings = {}\n",
        "feature_engineering_train_input = train_input.copy()\n",
        "\n",
        "print(\"\\n[2/4] Preparing with enhanced features...\")\n",
        "with timer(\"feature_engineering\", feature_engineering_timings):\n",
        "    feature_engineering_train_input, feature_cols, feature_engineering_timings = feature_engineering(feature_engineering_train_input, feature_engineering_timings)\n",
        "\n",
        "print_timing_summary(feature_engineering_timings, title=\"Feature Engineering Summary\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[2.5/4] Creating sequences...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "401e497cc31048dd909b3e345490515b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Creating sequences:   0%|          | 0/46045 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created 46021 sequences with 103 features each\n",
            "  [317.93s] create_sequences\n",
            "\n",
            "Sequence Creation Summary:\n",
            "============================================================\n",
            "  create_sequences         : 317.93s (100.0%)\n",
            "Total                    : 317.93s\n"
          ]
        }
      ],
      "source": [
        "sequence_timings = {}\n",
        "sequence_train_input = feature_engineering_train_input.copy()\n",
        "sequence_train_output = train_output.copy()\n",
        "\n",
        "print(\"\\n[2.5/4] Creating sequences...\")\n",
        "with timer(\"create_sequences\", sequence_timings):\n",
        "    sequences, targets_dx, targets_dy, targets_frame_ids, sequence_ids = create_sequences(\n",
        "        sequence_train_input, feature_cols, sequence_train_output, test_template=None, is_training=True, window_size=config.WINDOW_SIZE\n",
        "    )\n",
        "print_timing_summary(sequence_timings, title=\"Sequence Creation Summary\")\n",
        "\n",
        "sequences = np.array(sequences, dtype=object)\n",
        "targets_dx = np.array(targets_dx, dtype=object)\n",
        "targets_dy = np.array(targets_dy, dtype=object)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## K-Fold Training\n",
        "\n",
        "Train models with K-fold cross-validation. The training function handles single model training with early stopping.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ Reloaded pipeline modules\n"
          ]
        }
      ],
      "source": [
        "# Reload pipeline modules\n",
        "import importlib\n",
        "import pipeline.training\n",
        "importlib.reload(pipeline.training)\n",
        "importlib.reload(pipeline.save_model)\n",
        "from pipeline.training import train_model\n",
        "from pipeline.save_model import save_model_ensemble\n",
        "print(\"âœ“ Reloaded pipeline modules\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Fold 1/5\n",
            "  [2.91s] Fold 1 - Scaling\n",
            "  Epoch 10: train=0.0640, val=0.0625\n",
            "  Epoch 20: train=0.0496, val=0.0492\n",
            "  Epoch 30: train=0.0432, val=0.0449\n",
            "  Epoch 40: train=0.0390, val=0.0453\n",
            "  Epoch 50: train=0.0366, val=0.0449\n",
            "  Epoch 60: train=0.0358, val=0.0448\n",
            "  Epoch 70: train=0.0350, val=0.0447\n",
            "  Epoch 80: train=0.0349, val=0.0446\n",
            "  Epoch 90: train=0.0345, val=0.0446\n",
            "  Epoch 100: train=0.0344, val=0.0446\n",
            "  Early stop at epoch 101\n",
            "  [339.49s] Fold 1 - Train X model\n",
            "  Epoch 10: train=0.0543, val=0.0466\n",
            "  Epoch 20: train=0.0441, val=0.0471\n",
            "  Epoch 30: train=0.0368, val=0.0437\n",
            "  Epoch 40: train=0.0337, val=0.0435\n",
            "  Epoch 50: train=0.0318, val=0.0427\n",
            "  Epoch 60: train=0.0309, val=0.0427\n",
            "  Epoch 70: train=0.0309, val=0.0426\n",
            "  Early stop at epoch 77\n",
            "  [244.20s] Fold 1 - Train Y model\n",
            "Fold 1 - X loss: 0.04453, Y loss: 0.04255\n",
            "\n",
            "Fold 2/5\n",
            "  [2.52s] Fold 2 - Scaling\n",
            "  Epoch 10: train=0.0625, val=0.0566\n",
            "  Epoch 20: train=0.0536, val=0.0584\n",
            "  Epoch 30: train=0.0425, val=0.0522\n",
            "  Epoch 40: train=0.0371, val=0.0480\n",
            "  Epoch 50: train=0.0348, val=0.0487\n",
            "  Epoch 60: train=0.0336, val=0.0479\n",
            "  Epoch 70: train=0.0328, val=0.0479\n",
            "  Epoch 80: train=0.0325, val=0.0481\n",
            "  Epoch 90: train=0.0325, val=0.0479\n",
            "  Early stop at epoch 94\n",
            "  [291.18s] Fold 2 - Train X model\n",
            "  Epoch 10: train=0.0554, val=0.0535\n",
            "  Epoch 20: train=0.0483, val=0.0500\n",
            "  Epoch 30: train=0.0376, val=0.0468\n",
            "  Epoch 40: train=0.0316, val=0.0425\n",
            "  Epoch 50: train=0.0299, val=0.0425\n",
            "  Epoch 60: train=0.0290, val=0.0426\n",
            "  Epoch 70: train=0.0287, val=0.0424\n",
            "  Early stop at epoch 77\n",
            "  [236.79s] Fold 2 - Train Y model\n",
            "Fold 2 - X loss: 0.04766, Y loss: 0.04223\n",
            "\n",
            "Fold 3/5\n",
            "  [2.73s] Fold 3 - Scaling\n",
            "  Epoch 10: train=0.0599, val=0.0552\n",
            "  Epoch 20: train=0.0490, val=0.0525\n",
            "  Epoch 30: train=0.0438, val=0.0571\n",
            "  Epoch 40: train=0.0400, val=0.0490\n",
            "  Epoch 50: train=0.0363, val=0.0484\n",
            "  Epoch 60: train=0.0348, val=0.0482\n",
            "  Epoch 70: train=0.0340, val=0.0482\n",
            "  Epoch 80: train=0.0336, val=0.0479\n",
            "  Epoch 90: train=0.0336, val=0.0479\n",
            "  Epoch 100: train=0.0337, val=0.0479\n",
            "  Early stop at epoch 103\n",
            "  [307.86s] Fold 3 - Train X model\n",
            "  Epoch 10: train=0.0559, val=0.0464\n",
            "  Epoch 20: train=0.0438, val=0.0477\n",
            "  Epoch 30: train=0.0409, val=0.0449\n",
            "  Epoch 40: train=0.0350, val=0.0433\n",
            "  Epoch 50: train=0.0317, val=0.0433\n",
            "  Epoch 60: train=0.0304, val=0.0430\n",
            "  Epoch 70: train=0.0292, val=0.0428\n",
            "  Epoch 80: train=0.0284, val=0.0428\n",
            "  Early stop at epoch 83\n",
            "  [246.61s] Fold 3 - Train Y model\n",
            "Fold 3 - X loss: 0.04780, Y loss: 0.04261\n",
            "\n",
            "Fold 4/5\n",
            "  [2.49s] Fold 4 - Scaling\n",
            "  Epoch 10: train=0.0618, val=0.0568\n",
            "  Epoch 20: train=0.0502, val=0.0496\n",
            "  Epoch 30: train=0.0439, val=0.0452\n",
            "  Epoch 40: train=0.0413, val=0.0445\n",
            "  Epoch 50: train=0.0395, val=0.0440\n",
            "  Epoch 60: train=0.0388, val=0.0439\n",
            "  Epoch 70: train=0.0386, val=0.0438\n",
            "  Epoch 80: train=0.0383, val=0.0438\n",
            "  Epoch 90: train=0.0387, val=0.0438\n",
            "  Epoch 100: train=0.0381, val=0.0438\n",
            "  Early stop at epoch 108\n",
            "  [341.32s] Fold 4 - Train X model\n",
            "  Epoch 10: train=0.0547, val=0.0463\n",
            "  Epoch 20: train=0.0442, val=0.0431\n",
            "  Epoch 30: train=0.0399, val=0.0444\n",
            "  Epoch 40: train=0.0347, val=0.0442\n",
            "  Epoch 50: train=0.0320, val=0.0413\n",
            "  Epoch 60: train=0.0306, val=0.0409\n",
            "  Epoch 70: train=0.0300, val=0.0411\n",
            "  Epoch 80: train=0.0298, val=0.0409\n",
            "  Early stop at epoch 82\n",
            "  [255.00s] Fold 4 - Train Y model\n",
            "Fold 4 - X loss: 0.04374, Y loss: 0.04078\n",
            "\n",
            "Fold 5/5\n",
            "  [2.46s] Fold 5 - Scaling\n",
            "  Epoch 10: train=0.0602, val=0.0636\n",
            "  Epoch 20: train=0.0476, val=0.0517\n",
            "  Epoch 30: train=0.0434, val=0.0519\n",
            "  Epoch 40: train=0.0393, val=0.0491\n",
            "  Epoch 50: train=0.0375, val=0.0486\n",
            "  Epoch 60: train=0.0359, val=0.0486\n",
            "  Epoch 70: train=0.0352, val=0.0486\n",
            "  Epoch 80: train=0.0356, val=0.0485\n",
            "  Epoch 90: train=0.0350, val=0.0484\n",
            "  Epoch 100: train=0.0350, val=0.0485\n",
            "  Early stop at epoch 105\n",
            "  [313.99s] Fold 5 - Train X model\n",
            "  Epoch 10: train=0.0537, val=0.0520\n",
            "  Epoch 20: train=0.0474, val=0.0482\n",
            "  Epoch 30: train=0.0357, val=0.0440\n",
            "  Epoch 40: train=0.0329, val=0.0444\n",
            "  Epoch 50: train=0.0310, val=0.0441\n",
            "  Epoch 60: train=0.0306, val=0.0440\n",
            "  Epoch 70: train=0.0297, val=0.0440\n",
            "  Epoch 80: train=0.0302, val=0.0440\n",
            "  Epoch 90: train=0.0297, val=0.0441\n",
            "  Early stop at epoch 92\n",
            "  [272.09s] Fold 5 - Train Y model\n",
            "Fold 5 - X loss: 0.04837, Y loss: 0.04395\n",
            "\n",
            "============================================================\n",
            "Training Complete!\n",
            "\n",
            "Training Timing Summary:\n",
            "============================================================\n",
            "  Fold 4 - Train X model   : 341.32s ( 11.9%)\n",
            "  Fold 1 - Train X model   : 339.49s ( 11.9%)\n",
            "  Fold 5 - Train X model   : 313.99s ( 11.0%)\n",
            "  Fold 3 - Train X model   : 307.86s ( 10.8%)\n",
            "  Fold 2 - Train X model   : 291.18s ( 10.2%)\n",
            "  Fold 5 - Train Y model   : 272.09s (  9.5%)\n",
            "  Fold 4 - Train Y model   : 255.00s (  8.9%)\n",
            "  Fold 3 - Train Y model   : 246.61s (  8.6%)\n",
            "  Fold 1 - Train Y model   : 244.20s (  8.5%)\n",
            "  Fold 2 - Train Y model   : 236.79s (  8.3%)\n",
            "  Fold 1 - Scaling         :   2.91s (  0.1%)\n",
            "  Fold 3 - Scaling         :   2.73s (  0.1%)\n",
            "  Fold 2 - Scaling         :   2.52s (  0.1%)\n",
            "  Fold 4 - Scaling         :   2.49s (  0.1%)\n",
            "  Fold 5 - Scaling         :   2.46s (  0.1%)\n",
            "Total                    : 2861.65s\n",
            "\n",
            "Trained 5 X models and 5 Y models\n"
          ]
        }
      ],
      "source": [
        "groups = np.array([d['game_id'] for d in sequence_ids])\n",
        "gkf = GroupKFold(n_splits=config.N_FOLDS)\n",
        "\n",
        "models_x, models_y, scalers = [], [], []\n",
        "train_timings = {}\n",
        "\n",
        "for fold, (tr, va) in enumerate(gkf.split(sequences, groups=groups), 1):\n",
        "    print(f\"\\nFold {fold}/{config.N_FOLDS}\")\n",
        "    \n",
        "    X_tr, X_va = sequences[tr], sequences[va]\n",
        "    \n",
        "    # Scale features\n",
        "    with timer(f\"Fold {fold} - Scaling\", train_timings):\n",
        "        scaler = StandardScaler()\n",
        "        scaler.fit(np.vstack([s for s in X_tr]))\n",
        "        X_tr_sc = np.stack([scaler.transform(s) for s in X_tr])\n",
        "        X_va_sc = np.stack([scaler.transform(s) for s in X_va])\n",
        "    \n",
        "    # Train X model\n",
        "    with timer(f\"Fold {fold} - Train X model\", train_timings):\n",
        "        mx, loss_x = train_model(\n",
        "            X_tr_sc, targets_dx[tr], X_va_sc, targets_dx[va],\n",
        "            X_tr[0].shape[-1], config.MAX_FUTURE_HORIZON, config\n",
        "        )\n",
        "    \n",
        "    # Train Y model\n",
        "    with timer(f\"Fold {fold} - Train Y model\", train_timings):\n",
        "        my, loss_y = train_model(\n",
        "            X_tr_sc, targets_dy[tr], X_va_sc, targets_dy[va],\n",
        "            X_tr[0].shape[-1], config.MAX_FUTURE_HORIZON, config\n",
        "        )\n",
        "    \n",
        "    # Save for ensemble\n",
        "    models_x.append(mx)\n",
        "    models_y.append(my)\n",
        "    scalers.append(scaler)\n",
        "    \n",
        "    print(f\"Fold {fold} - X loss: {loss_x:.5f}, Y loss: {loss_y:.5f}\")\n",
        "    \n",
        "# Print training summary\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Training Complete!\")\n",
        "print_timing_summary(train_timings, title=\"Training Timing Summary\")\n",
        "print(f\"\\nTrained {len(models_x)} X models and {len(models_y)} Y models\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Saving\n",
        "\n",
        "Save the trained ensemble when satisfied with results. Automatic versioning handles model organization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "âœ“ Model saved to: /Users/matth/projects/nfl-big-data-bowl-2026-prediction/models/nn_baseline.0\n",
            "Model saving would go here\n"
          ]
        }
      ],
      "source": [
        "# Save trained models\n",
        "# TODO: Uncomment when models are trained\n",
        "from datetime import datetime\n",
        "metadata = {\n",
        "    'feature_names': feature_cols,  # Make sure feature_cols is in scope\n",
        "    'num_features': len(feature_cols),\n",
        "    'training_date': datetime.now().isoformat(),\n",
        "    'num_folds': len(models_x),\n",
        "    'num_sequences': len(sequences),\n",
        "    # Note: validation_losses would need to be manually entered or left out\n",
        "}\n",
        "\n",
        "save_model_ensemble(\n",
        "    models_x, models_y, scalers, config, metadata,\n",
        "    model_id='nn_baseline'\n",
        ")\n",
        "\n",
        "print(\"Model saving would go here\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Submission Server\n",
        "\n",
        "Create a submission server for Kaggle competition API.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define prediction function for submission API\n",
        "def predict_fn(test: pl.DataFrame, test_input: pl.DataFrame) -> pl.DataFrame:\n",
        "    \"\"\"\n",
        "    Prediction function for Kaggle competition API.\n",
        "    \n",
        "    This function receives test data in batches and returns predictions.\n",
        "    It should:\n",
        "    1. Load saved models\n",
        "    2. Transform test data (feature engineering, scaling)\n",
        "    3. Generate predictions\n",
        "    4. Return Polars DataFrame with x, y columns\n",
        "    \"\"\"\n",
        "    # TODO: Implement prediction logic\n",
        "    # predictions = pl.DataFrame({'x': [0.0] * len(test), 'y': [0.0] * len(test)})\n",
        "    return pl.DataFrame({'x': [0.0] * len(test), 'y': [0.0] * len(test)})\n",
        "\n",
        "# Create submission server\n",
        "# server = create_submission_server(predict_fn)\n",
        "\n",
        "# For local testing:\n",
        "# server = create_submission_server(predict_fn, gateway_path=('/kaggle/input/nfl-big-data-bowl-2026-prediction/',))\n",
        "\n",
        "print(\"Submission server would be set up here\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
